version: "3.9"

services:
  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    volumes:
      - whisper-socket:/app/sockets
      - llama-socket:/app/llama-sockets
      - piper-socket:/app/piper-sockets
      - f5-socket:/app/f5-sockets                  # ➊ NEW
    ports:
      - "8090:8090"
    depends_on:
      whisper:
        condition: service_healthy
      llama:
        condition: service_started
      piper:
        condition: service_healthy
      f5-tts:                                      # ➋ NEW
        condition: service_healthy
    restart: on-failure
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for whisper socket…' &&
      while [ ! -S /app/sockets/whisper.sock ]; do sleep 0.2; done &&
      echo 'Waiting for llama socket…' &&
      while [ ! -S /app/llama-sockets/llama.sock ]; do sleep 0.2; done &&
      echo 'Waiting for piper socket…' &&
      while [ ! -S /app/piper-sockets/piper.sock ]; do sleep 0.2; done &&
      echo 'Waiting for F5 socket…' &&                             # ➌ NEW
      while [ ! -S /app/f5-sockets/f5-tts.sock ]; do sleep 0.2; done &&
      echo 'All backends ready; starting API…' &&
      exec /app/server"

  whisper:
    build:
      context: .
      dockerfile: whisper/Dockerfile
      args: [CUDA_ARCHS=89]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - WHISPER_MODEL=kb-ggml-model.bin
    volumes:
      - whisper-socket:/app/sockets
    ulimits:
      memlock: -1          # allow page-locked (“pinned”) memory
      stack:   67108864    # 64 MiB stack – CUDA’s usual default
    healthcheck:
      test: ["CMD", "test", "-S", "/app/sockets/whisper.sock"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure

  llama:
    build:
      context: .
      dockerfile: llama_server/Dockerfile
      args: [CUDA_ARCHS=89]
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      LLAMA_N_CTX: 2048
      LLAMA_N_GPU_LAYERS: 33
      GGML_CUDA_FORCE_MMQ: "1"
    volumes:
      - llama-socket:/app/llama-sockets
      - ./llama_models:/app/models
    ulimits:
      memlock: -1
      stack:   67108864
    healthcheck:
      test: ["CMD", "test", "-S", "/app/llama-sockets/llama.sock"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure

  piper:
    build:
      context: .
      dockerfile: piper_server/Dockerfile
      args: [CUDA_ARCHS=89]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - piper-socket:/app/piper-sockets
      - ./piper_models:/app/models
    ulimits:
      memlock: -1
      stack:   67108864
    healthcheck:
      test: ["CMD", "test", "-S", "/app/piper-sockets/piper.sock"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure

  # ─────────────────────────────────────────────────────────────────────────
  # NEW GPU F5-TTS SERVICE
  # ─────────────────────────────────────────────────────────────────────────
  f5-tts:
    build:
      context: .
      dockerfile: f5_server/Dockerfile      # points to the GPU Dockerfile
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - f5-socket:/app/f5-sockets
    ulimits:
      memlock: -1
      stack:   67108864
    healthcheck:
      test: ["CMD", "test", "-S", "/app/f5-sockets/f5-tts.sock"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure

volumes:
  whisper-socket:
  llama-socket:
  piper-socket:
  f5-socket:                                      # ➍ NEW
  whisper-models: