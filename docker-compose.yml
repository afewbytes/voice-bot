# docker-compose.yml
version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    volumes:
      - whisper-socket:/app/sockets
    ports:
      - "8090:8090"       # gRPC API port
    depends_on:
      whisper:
        condition: service_healthy
    restart: on-failure
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for whisper service to be ready...' &&
      while [ ! -f /app/sockets/ready ]; do sleep 0.5; done &&
      echo 'Whisper service is ready, starting API...' &&
      exec /app/server"

  whisper:
    build:
      context: .
      dockerfile: whisper/Dockerfile
    volumes:
      - whisper-socket:/app/sockets
      - whisper-models:/app/models
    environment:
      - WHISPER_MODEL=ggml-base.en.bin
      - WHISPER_USE_GPU=0
    healthcheck:
      test: ["CMD", "test", "-f", "/app/sockets/ready"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 5G

  llama:
    build:
      context: .
      dockerfile: llama_server/Dockerfile
      args:
        - LLAMA_MODEL=llama-2-7b.Q4_K_M.gguf
    environment:
      - LLAMA_MODEL=llama-2-7b.Q4_K_M.gguf
    volumes:
      - ./llama_models:/app/models
    ports:
      - "50051:50051"     # gRPC LLaMA port
    depends_on:
      api:
        condition: service_started
    restart: on-failure

volumes:
  whisper-socket:
    driver: local
  whisper-models:
    driver: local
  llama-models:
    driver: local