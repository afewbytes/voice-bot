version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    volumes:
      - whisper-socket:/app/sockets
      - llama-socket:/app/llama-sockets
    ports:
      - "8090:8090"       # external gRPC API
    depends_on:
      whisper:
        condition: service_healthy
      llama:
        condition: service_started
    restart: on-failure
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for whisper socket…' &&
      while [ ! -S /app/sockets/whisper.sock ]; do sleep 0.2; done &&
      echo 'Waiting for llama socket…' &&
      while [ ! -S /app/llama-sockets/llama.sock ]; do sleep 0.2; done &&
      echo 'All backends ready; starting API…' &&
      exec /app/server"

  whisper:
    build:
      context: .
      dockerfile: whisper/Dockerfile
    volumes:
      - whisper-socket:/app/sockets
      - whisper-models:/app/models
    environment:
      - WHISPER_MODEL=ggml-base.en.bin
      - WHISPER_USE_GPU=0
    healthcheck:
      test: ["CMD", "test", "-S", "/app/sockets/whisper.sock"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure

  llama:
    build:
      context: .
      dockerfile: llama_server/Dockerfile
      args:
        - LLAMA_MODEL=llama-2-7b.Q4_K_M.gguf
    volumes:
      - llama-socket:/app/llama-sockets
      - ./llama_models:/app/models      # bind-mount your local models
    healthcheck:
      test: ["CMD", "test", "-S", "/app/llama-sockets/llama.sock"]
      interval: 2s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: on-failure

volumes:
  whisper-socket:
    driver: local
  llama-socket:
    driver: local
  whisper-models:
    driver: local